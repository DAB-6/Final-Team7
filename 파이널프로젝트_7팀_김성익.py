# -*- coding: utf-8 -*-
"""파이널_김성익.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17rV64vuNxTweky0uec6bFtBsL0q0B4ut
"""

import pandas as pd

df = pd.read_csv('/content/dairy_dataset.csv')

df['Date']= pd.to_datetime(df['Date'])
df['Production Date'] = pd.to_datetime(df['Production Date'])
df['Expiration Date'] = pd.to_datetime(df['Expiration Date'])

# 평균 유통기한
avg_Shelf_life = df.groupby('Product Name')['Shelf Life (days)'].mean().reset_index()
avg_Shelf_life

df.columns = df.columns.str.replace('(liters/kg)', '', regex=False)
df.columns = df.columns.str.replace('(days)', '', regex=False)
df.columns = df.columns.str.replace('(acres)', '', regex=False)
df.columns = df.columns.str.replace('(sold)', 'Sold', regex=False)
df.columns = df.columns.str.replace('(INR)', '', regex=False)
df.columns = df.columns.str.replace('.', '', regex=False)
df.columns = df.columns.str.rstrip()
df = df.rename(columns=lambda x: x.replace(' ', '_'))

import numpy as np
import pandas as pd

df['Sales_Period_Days'] = (df['Date'] - df['Production_Date']).dt.days
df["Avg_Daily_Sales"] = df["Quantity_Sold"] / (df["Date"] - df["Production_Date"]).dt.days
df['Remaining_Shelf_Life'] = (df['Expiration_Date'] - df['Date']).dt.days

df['Expected_Waste_Qty'] = np.where(
    df['Remaining_Shelf_Life'] <= 0,
    df['Quantity_in_Stock'],
    (df['Quantity_in_Stock'] - (df['Avg_Daily_Sales'] * df['Remaining_Shelf_Life'])).clip(lower=0)
)

df['Expected_Waste_Qty'] = df['Expected_Waste_Qty'].fillna(df['Quantity_in_Stock'])
df['Waste_Rate'] = df['Expected_Waste_Qty'] / df['Quantity']

df.drop('Farm_Size', axis=1, inplace=True)

# Commented out IPython magic to ensure Python compatibility.
# #한글 글씨 폰트 설치
# %%capture
# !sudo apt-get install -y fonts-nanum
# !sudo fc-cache -fv
# !rm ~/.cache/matplotlib -rf
# 
# import matplotlib.pyplot as plt
# import matplotlib.font_manager as fm
# fm.fontManager.addfont('/usr/share/fonts/truetype/nanum/NanumGothic.ttf')
# plt.rcParams['font.family'] = 'NanumGothic'

df.columns

# 생산량(Quantity)과 폐기량(Expected_Waste_Qty) 비교
prod_vs_EXQ = df.groupby("Brand")[["Quantity", "Expected_Waste_Qty"]].sum()

# 바그래프 출력
prod_vs_EXQ.plot(kind="bar", figsize=(12,6))
plt.title("브랜드별 생산량과 폐기량 비교")
plt.ylabel("수량 (liters/kg)")
plt.xlabel("제품명")
plt.xticks(rotation=45)
plt.legend(["생산량", "폐기량"])
plt.tight_layout()
plt.show()

# 생산량(Quantity)과 폐기량(Expected_Waste_Qty) 비교
prod_Nvs_EWQ = df.groupby("Product_Name")[["Quantity", "Expected_Waste_Qty"]].sum()

# 바그래프 출력
prod_Nvs_EWQ.plot(kind="bar", figsize=(12,6))
plt.title("제품별 생산량과 폐기량 비교")
plt.ylabel("수량 (liters/kg)")
plt.xlabel("제품명")
plt.xticks(rotation=45)
plt.legend(["생산량", "폐기량"])
plt.tight_layout()
plt.show()

# 특정 브랜드별 생산량 vs 폐기량 비교
B_Q_EWQ = df[df["Brand"] == "Amul"].groupby("Product_Name")[["Quantity", "Expected_Waste_Qty"]].sum()

B_Q_EWQ.plot(kind="bar", figsize=(12,6))
plt.title("Amul의 제품별 생산량과 폐기량 비교")
plt.ylabel("수량 (liters/kg)")
plt.xlabel("제품명")
plt.xticks(rotation=45)
plt.legend(["생산량", "폐기량"])
plt.tight_layout()
plt.show()

df["Shelf_Life_Days"] = (df["Expiration_Date"] - df["Production_Date"]).dt.days

import statsmodels.api as sm
from statsmodels.formula.api import ols

model = ols('Q("Waste_Rate") ~ C(Brand)', data=df).fit()

anova_table = sm.stats.anova_lm(model, typ=2)

anova_table

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12,6))
sns.boxplot(x="Brand", y="Waste_Rate", data=df)  # Target_Variable은 분석한 변수명으로 교체
plt.xticks(rotation=45)
plt.title("브랜드별 분산 차이 (Boxplot)")
plt.show()

"""독립: Brand
오차항(Residual)   

F = 7.043036   
집단 간 변동이 집단 내 변동보다 7배 큼, p = 4.581461e-11 으로 p <=0.05 브랜드 별 평균 차이는 유의하다라고 할 수 있다.
"""

# 브랜드별 제품군 groupby
df_grouped = df.groupby(["Brand","Product_Name"], as_index=False)["Waste_Rate"].mean()

model01 = ols("Waste_Rate ~ C(Brand) + C(Product_Name)", data=df_grouped).fit()
anova_table01 = sm.stats.anova_lm(model01, typ=2)
anova_table01

df_grouped

df.columns

df_grouped02 = df.groupby(["Brand","Product_Name", "Shelf_Life_Days"], as_index=False)["Waste_Rate"].mean()

# 상호작(x) 가정
model02 = ols("Waste_Rate ~ C(Brand) + C(Product_Name) + (Shelf_Life_Days)", data=df_grouped02).fit()
anova_table02 = sm.stats.anova_lm(model02, typ=2)
anova_table02

df_grouped02 = df.groupby(["Brand","Product_Name", "Shelf_Life_Days"], as_index=False)["Waste_Rate"].mean()

# 상호작용 포함(브랜드와 제품군) + 유통기한의 공변량 효과
model03 = ols("Waste_Rate ~ C(Brand) * C(Product_Name) + (Shelf_Life_Days)", data=df_grouped02).fit()
anova_table03 = sm.stats.anova_lm(model03, typ=3)
anova_table03

# 유통기한의 상호작용도 포함
model04 = ols("Waste_Rate ~ C(Brand) * C(Product_Name) * (Shelf_Life_Days)", data=df_grouped02).fit()
anova_table04 = sm.stats.anova_lm(model04, typ=3)
anova_table04

import pandas as pd, numpy as np
import seaborn as sns, matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf
from statsmodels.stats.anova import anova_lm
from statsmodels.stats.multicomp import pairwise_tukeyhsd

# 0) 치즈만 필터 + 결측 제거
cheese = (df
          .loc[df['Product_Name']=='Cheese', ['Waste_Rate','Brand','Storage_Condition']]
          .dropna())

# (선택) 셀 크기 확인: 각 Brand×Storage 조합 표본수
cell_sizes = (cheese
              .groupby(['Brand','Storage_Condition'])
              .size().rename('n').reset_index())
print("[셀 크기]\n", cell_sizes.sort_values('n'))

# 1) 이원 ANOVA (상호작용 포함)
m = smf.ols('Waste_Rate ~ C(Brand) * C(Storage_Condition)', data=cheese).fit()
aov = anova_lm(m, typ=2)  # Type II ANOVA
print("\n[이원 ANOVA]\n", aov)

# (참고) 효과크기 η² / partial η²
ss_total = aov['sum_sq'].sum()
ss_resid = aov.loc['Residual','sum_sq']
eff = aov.drop(index='Residual').copy()
eff['eta2'] = eff['sum_sq'] / ss_total
eff['partial_eta2'] = eff['sum_sq'] / (eff['sum_sq'] + ss_resid)
print("\n[효과크기]\n", eff[['eta2','partial_eta2']].round(4))

# 2) 사후분석 (모든 Brand×Storage 조합 간 Tukey HSD)
cheese['group'] = cheese['Brand'].astype(str) + ' | ' + cheese['Storage_Condition'].astype(str)
tukey = pairwise_tukeyhsd(endog=cheese['Waste_Rate'], groups=cheese['group'], alpha=0.05)
print("\n[Tukey HSD 요약]\n", tukey.summary())

# --- 시각화 ---

# A) 상호작용 플롯: 브랜드 라인, x=보관방법, y=평균 폐기율(±95% CI)
plt.figure(figsize=(9,4.2))
sns.pointplot(
    data=cheese, x='Storage_Condition', y='Waste_Rate', hue='Brand',
    dodge=0.3, errorbar=('ci',95)
)
plt.title('Cheese: Brand × Storage에 따른 평균 폐기율 (±95% CI)')
plt.ylabel('Waste_Rate'); plt.xlabel('Storage_Condition')
plt.grid(axis='y', alpha=0.3); plt.tight_layout(); plt.show()

# B) 박스플롯(분포 확인): x=보관방법, col=브랜드
g = sns.catplot(
    data=cheese, kind='box', x='Storage_Condition', y='Waste_Rate',
    col='Brand', col_wrap=4, sharey=True, height=3.2, showfliers=False
)
g.set_titles('{col_name}'); g.set_xlabels('Storage_Condition'); g.set_ylabels('Waste_Rate')
plt.tight_layout(); plt.show()

# C) Tukey 동시신뢰구간 플롯 (쌍별 평균차)
fig = tukey.plot_simultaneous(figsize=(10,6))
plt.title('Tukey HSD: Brand|Storage 쌍별 평균 차 (95% 동시 CI)')
plt.xlabel('평균 차이 (Waste_Rate)'); plt.grid(axis='x', alpha=0.3)
plt.tight_layout(); plt.show()

# 브랜드별 제품군 groupby
df_grouped = df.groupby(["Product_Name", "Shelf_Life_Days"], as_index=False)["Waste_Rate"].mean()

model01 = ols("Waste_Rate ~ C(Product_Name) + C(Shelf_Life_Days)", data=df_grouped).fit()
anova_table01 = sm.stats.anova_lm(model01, typ=2)
anova_table01

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# === 1) ANOVA 결과를 DataFrame으로 구성 (질문에서 준 값) ===
aov = pd.DataFrame({
    "term": ["C(Product_Name)", "C(Shelf_Life_Days)", "Residual"],
    "sum_sq": [0.012199, 3.066674, 0.960429],
    "df":     [9.0,        145.0,      77.0],
    "F":      [0.108668,   1.695606,   np.nan],
    "p":      [0.999401,   0.005548,   np.nan],   # PR(>F)
})

# === 2) 효과크기(eta^2, partial eta^2) 계산 ===
ss_total = aov["sum_sq"].sum()
ss_resid = aov.loc[aov["term"]=="Residual", "sum_sq"].values[0]

effects = aov[aov["term"]!="Residual"].copy()
effects["eta2"] = effects["sum_sq"] / ss_total
effects["partial_eta2"] = effects["sum_sq"] / (effects["sum_sq"] + ss_resid)

# === 3) –log10(p) 계산 (Residual 제외) ===
effects["neglog10_p"] = -np.log10(effects["p"])

# === 4) 시각화 ===
plt.figure(figsize=(12,3.6))

# (A) 효과크기 eta^2
plt.subplot(1,3,1)
plt.barh(effects["term"], effects["eta2"])
plt.title("효과크기 η²")
plt.xlabel("η² (SS_effect / SS_total)")
for y, v in enumerate(effects["eta2"]):
    plt.text(v + 0.005, y, f"{v:.3f}", va="center")
plt.xlim(0, max(0.05, effects["eta2"].max()*1.2))

# (B) 유의성: –log10(p) (α=0.05 선)
plt.subplot(1,3,2)
plt.barh(effects["term"], effects["neglog10_p"])
alpha = 0.05
thr = -np.log10(alpha)     # ≈ 1.301
plt.axvline(thr, ls="--", lw=1, color="gray")
plt.title("유의성 –log10(p)")
plt.xlabel("–log10(p)  (점선: α=0.05)")
for y, v in enumerate(effects["neglog10_p"]):
    plt.text(v + 0.02, y, f"p={effects['p'].iloc[y]:.3g}", va="center")
plt.xlim(0, max(thr*1.3, effects["neglog10_p"].max()*1.2))

# (C) F-statistics
plt.subplot(1,3,3)
plt.barh(effects["term"], effects["F"])
plt.title("F 통계량")
plt.xlabel("F")
for y, v in enumerate(effects["F"]):
    plt.text(v + 0.05, y, f"{v:.3f}", va="center")
plt.xlim(0, max(2.0, effects["F"].max()*1.2))

plt.tight_layout()
plt.show()

import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns
import statsmodels.api as sm
import statsmodels.formula.api as smf

# 일원 ANOVA: Waste_Rate ~ C(Brand)
m1 = smf.ols('Waste_Rate ~ C(Brand)', data=df).fit()
anova1 = sm.stats.anova_lm(m1, typ=2)
print(anova1)

# 집단별 평균/표준오차/95% CI
g = df.groupby('Brand')['Waste_Rate']
summary = pd.DataFrame({
    'mean': g.mean(),
    'se': g.std(ddof=1)/np.sqrt(g.count()),
    'n': g.count()
})
summary['ci95'] = 1.96 * summary['se']
summary = summary.reset_index()

# 시각화: 평균±CI (막대 or 포인트)
plt.figure(figsize=(9,4))
sns.pointplot(data=summary, x='Brand', y='mean', join=False, errorbar=None)
plt.errorbar(x=np.arange(len(summary)), y=summary['mean'],
             yerr=summary['ci95'], fmt='none', capsize=4, lw=1.4, color='C0')
plt.title('일원 ANOVA: Brand별 평균 폐기율(±95% CI)')
plt.ylabel('Waste_Rate')
plt.xticks(rotation=45, ha='right')
plt.grid(axis='y', alpha=0.3); plt.tight_layout(); plt.show()

# 분포도 함께: 상자그림/바이올린
plt.figure(figsize=(9,4))
sns.boxplot(data=df, x='Brand', y='Waste_Rate', showfliers=False)
sns.stripplot(data=df, x='Brand', y='Waste_Rate', color='k', alpha=0.25, size=2)
plt.title('Brand별 폐기율 분포 (Box + Strip)'); plt.xticks(rotation=45, ha='right')
plt.tight_layout(); plt.show()

plt.figure(figsize=(9,4))
sns.pointplot(data=summary, y='Brand', x='mean', join=False, errorbar=None, orient='h')
# 에러바도 가로로
plt.errorbar(y=np.arange(len(summary)), x=summary['mean'],
             xerr=summary['ci95'], fmt='none', capsize=4, lw=1.4, color='C0')
plt.title('일원 ANOVA: Brand별 평균 폐기율(±95% CI)')
plt.xlabel('Waste_Rate')
plt.ylabel('Brand')
plt.grid(axis='x', alpha=0.3)
plt.tight_layout(); plt.show()

from scipy.stats import ttest_ind

# 예시: 특정 두 집단(A,B)의 차이 p값을 박스플롯에 표기
A, B = 'Amul', 'Mother Dairy'
yA = df.loc[df['Brand']==A, 'Waste_Rate'].dropna()
yB = df.loc[df['Brand']==B, 'Waste_Rate'].dropna()
_, p = ttest_ind(yA, yB, equal_var=False)

plt.figure(figsize=(4.2,4))
sns.boxplot(data=df[df['Brand'].isin([A,B])], x='Brand', y='Waste_Rate', showfliers=False)
# 주석
ymax = df.loc[df['Brand'].isin([A,B]), 'Waste_Rate'].max()
plt.plot([0,1], [ymax*1.05, ymax*1.05], color='k')
plt.text(0.5, ymax*1.08, f'p={p:.3g}', ha='center')
plt.title(f'{A} vs {B}'); plt.tight_layout(); plt.show()

# 브랜드별 제품군 groupby
df_grouped = df.groupby(["Product_Name", "Shelf_Life_Days"], as_index=False)["Waste_Rate"].mean()

model01 = ols("Waste_Rate ~ C(Product_Name) + (Shelf_Life_Days)", data=df_grouped).fit()
anova_table01 = sm.stats.anova_lm(model01, typ=2)
anova_table01

# 산점도
sns.scatterplot(data=df, x="Shelf_Life_Days", y="Waste_Rate", alpha=0.6)

# 선형 회귀선 (95% 신뢰구간 포함)
sns.regplot(data=df, x="Shelf_Life_Days", y="Waste_Rate", scatter=False, ci=95, color="red")

plt.title("유통기한 vs 폐기율 (+ 회기선)", fontsize=14)
plt.xlabel("Shelf Life Days")
plt.ylabel("Waste Rate")
plt.grid(alpha=0.3)
plt.show()

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

sns.scatterplot(data=df, x="Shelf_Life_Days", y="Waste_Rate", alpha=0.6)
sns.regplot(data=df, x="Shelf_Life_Days", y="Waste_Rate", scatter=False, ci=95, color="red")

plt.xscale('log')   # x 로그
# plt.yscale('log') # y도 로그로 보고 싶으면 주석 해제
plt.title("유통기한 vs 폐기율 (+ 회귀선) [로그 스케일 축]", fontsize=14)
plt.xlabel("Shelf Life Days (log scale)")
plt.ylabel("Waste Rate")
plt.grid(alpha=0.3, which='both')
plt.show()

mapping = {"Frozen": 0, "Tetra Pack": 1, "Refrigerated": 2, "Polythene Packet" : 3, "Ambient" : 4}

# 수치형 데이터만 선택
numeric_df = df.select_dtypes(include=[np.number])

plt.rcParams['font.family'] = 'NanumGothic'   # 없으면 'AppleGothic', 'Malgun Gothic' 등으로 바꿔보세요
plt.rcParams['axes.unicode_minus'] = False    # 마이너스 깨짐 방지

name_map = {
    'Location': '농장위치',
    'Total_Land_Area': '토지면적',
    'Number_of_Cows': '소(마리)수',
    'Date': '기록일자',
    'Customer_Location' : '고객위치',
    'Sales_Channel': '판매채널',
    'Product_ID': '제품고유번호',
    'Product_Name': '제품명',
    'Brand': '브랜드',
    'Quantity': '생산량',
    'Total_Value' : '총가치',
    'Quantity_Sold': '판매량',
    'Price_per_Unit_Sold': '판매가',
    'Quantity_in_Stock': '재고량',
    'Price_per_Unit': '단가',
    'Approx_Total_Revenue': '총수익',
    'Minimum_Stock_Threshold': '최소재고기준치',
    'Reorder_Quantity': '재주문기준치',
    'Shelf_Life': '유통기한',
    'Shelf_Life_Days': '유통기한(일)',
    'Waste_Rate': '폐기율',
    'Prod_per_Acre': '면적당생산',
    'Sales_Period_Days': '판매기간(일)',
    'Avg_Daily_Sales': '일평균판매량',
    'Remaining_Shelf_Life': '잔여유통기한',
    'Sales_Period_Days': '잔여판매일',
    'Expected_Waste_Qty': '예상폐기량'
}

# 피어슨 상관계수 계산
corr = numeric_df.corr(method="pearson").rename(index=name_map, columns=name_map)

# 히트맵 시각화 (-1 ~ 1 고정)
plt.figure(figsize=(12,8))
sns.heatmap(
    corr,
    annot=True, fmt=".2f",
    cmap="coolwarm",
    cbar=True,
    vmin=-1, vmax=1   # 색상 범위를 -1~1로 고정
)

plt.title("피어슨 상관계수 히트맵 (-1 ~ 1)", fontsize=14)
plt.show()

import numpy as np
import pandas as pd
from scipy import stats
import statsmodels.formula.api as smf

# 1) 전체: Spearman (비선형/단조 검출)
rho, p = stats.spearmanr(df['Shelf_Life_Days'], df['Waste_Rate'], nan_policy='omit')
print("Spearman ρ (전체):", rho, "p:", p)

# 2) 집단별: 보관조건/제품별 Spearman
grp = (df
       .dropna(subset=['Shelf_Life_Days','Waste_Rate'])
       .groupby(['Product_Name','Storage_Condition']))
res = grp.apply(lambda g: stats.spearmanr(g['Shelf_Life_Days'], g['Waste_Rate']).correlation) \
         .rename('spearman_rho').reset_index()
print(res.sort_values('spearman_rho'))

# 3) 로그-선형 회귀: y ~ log(x)
df2 = df.dropna(subset=['Shelf_Life_Days','Waste_Rate']).copy()
df2['log_shelf'] = np.log1p(df2['Shelf_Life_Days'])
m1 = smf.ols('Waste_Rate ~ log_shelf', data=df2).fit()
print(m1.summary())

# 4) 구간(피스와이즈) 회귀: 임계점(예: 30일) 전/후 기울기 비교
cut = 30
df2['post'] = (df2['Shelf_Life_Days'] > cut).astype(int)
df2['slope_post'] = df2['post'] * (df2['Shelf_Life_Days'] - cut)
m2 = smf.ols('Waste_Rate ~ Shelf_Life_Days + post + slope_post', data=df2).fit()
print(m2.summary())

import numpy as np, pandas as pd
import seaborn as sns, matplotlib.pyplot as plt
import statsmodels.formula.api as smf

df1 = df.dropna(subset=['Shelf_Life_Days','Waste_Rate']).copy()
df1['log_shelf'] = np.log1p(df1['Shelf_Life_Days'])

# 회귀 적합
m_log = smf.ols('Waste_Rate ~ log_shelf', data=df1).fit()

# 예측선용 그리드
xg = np.linspace(df1['Shelf_Life_Days'].min(), df1['Shelf_Life_Days'].max(), 200)
pred = m_log.predict(pd.DataFrame({'log_shelf': np.log1p(xg)}))

# 그림
plt.figure(figsize=(8,5))
sns.scatterplot(data=df1, x='Shelf_Life_Days', y='Waste_Rate', alpha=0.45, s=18)
plt.plot(xg, pred, color='red', linewidth=2.5, label='로그-선형 회귀선')

plt.xscale('log')
plt.title('유통기한 vs 폐기율 (로그-선형 회귀선)', fontsize=13)
plt.xlabel('Shelf Life Days (log scale)')
plt.ylabel('Waste Rate')
plt.grid(True, which='both', alpha=0.3)
plt.legend()
plt.tight_layout(); plt.show()

cut = 30
d = df1.copy()
d['post'] = (d['Shelf_Life_Days'] > cut).astype(int)
d['slope_post'] = d['post'] * (d['Shelf_Life_Days'] - cut)

m_pw = smf.ols('Waste_Rate ~ Shelf_Life_Days + post + slope_post', data=d).fit()

# 구간별 직선 생성
x1 = np.linspace(d['Shelf_Life_Days'].min(), cut, 100)
x2 = np.linspace(cut, d['Shelf_Life_Days'].max(), 100)

def pred_pw(x):
    post = (x > cut).astype(int)
    slope_post = post * (x - cut)
    X = pd.DataFrame({'Shelf_Life_Days': x, 'post': post, 'slope_post': slope_post})
    return m_pw.predict(X)

plt.figure(figsize=(8,5))
sns.scatterplot(data=d, x='Shelf_Life_Days', y='Waste_Rate', alpha=0.35, s=16)
plt.plot(x1, pred_pw(x1), color='orange', lw=3, label='Piecewise (≤30d)')
plt.plot(x2, pred_pw(x2), color='orange', lw=3, label='Piecewise (>30d)')
plt.axvline(cut, ls='--', color='gray', alpha=0.6)

plt.title('유통기한 vs 폐기율 (30일 기준 피스와이즈)', fontsize=13)
plt.xlabel('Shelf Life Days'); plt.ylabel('Waste Rate')
plt.grid(alpha=0.3); plt.legend()
plt.tight_layout(); plt.show()

from scipy import stats

# 그룹별 Spearman ρ
g = (df.dropna(subset=['Shelf_Life_Days','Waste_Rate'])
       .groupby(['Product_Name','Storage_Condition']))
res = g.apply(lambda t: stats.spearmanr(t['Shelf_Life_Days'], t['Waste_Rate']).correlation) \
       .rename('spearman_rho').reset_index()

# 정렬 & 시각화
res['seg'] = res['Product_Name'].astype(str) + ' | ' + res['Storage_Condition'].astype(str)
res = res.sort_values('spearman_rho')

plt.figure(figsize=(8,6))
sns.barplot(data=res, y='seg', x='spearman_rho', orient='h', color='cornflowerblue')
plt.axvline(0, color='k', lw=1)
plt.title('세그먼트별 Spearman ρ (유통기한 vs 폐기율)', fontsize=13)
plt.xlabel('Spearman ρ'); plt.ylabel('제품 | 보관조건')
plt.tight_layout(); plt.show()

from scipy import stats

# 그룹별 Spearman ρ
g = (df.dropna(subset=['Shelf_Life_Days','Waste_Rate'])
       .groupby(['Product_Name','Storage_Condition']))
res = g.apply(lambda t: stats.spearmanr(t['Shelf_Life_Days'], t['Waste_Rate']).correlation) \
       .rename('spearman_rho').reset_index()

# 정렬 & 라벨용 문자열
res['seg'] = res['Product_Name'].astype(str) + ' | ' + res['Storage_Condition'].astype(str)
res = res.sort_values('spearman_rho').reset_index(drop=True)

plt.figure(figsize=(8,6))
ax = sns.barplot(data=res, y='seg', x='spearman_rho', orient='h', color='cornflowerblue')
plt.axvline(0, color='k', lw=1)
plt.title('유통기한과 폐기율의 상관계수', fontsize=13) # 세그먼트별 Spearman ρ (유통기한 vs 폐기율)
plt.xlabel('Spearman ρ'); plt.ylabel('제품 | 보관조건')

# ── 값 주석 추가 ──
for p, v in zip(ax.patches, res['spearman_rho']):
    x = p.get_width()
    y = p.get_y() + p.get_height()/2
    # 값이 음수면 막대 왼쪽, 양수면 오른쪽에 살짝 띄움
    ha = 'right' if x < 0 else 'left'
    offset = -0.01 if x < 0 else 0.01
    ax.text(x + offset, y, f"{v:.3f}", va='center', ha=ha, fontsize=9)

# 값 표기를 위한 좌우 여백
xmin = min(0, res['spearman_rho'].min()) - 0.05
xmax = max(0, res['spearman_rho'].max()) + 0.05
ax.set_xlim(xmin, xmax)



plt.tight_layout(); plt.show()

# 상관이 상대적으로 큰 상위 4개 세그먼트 선택
top4 = (res.assign(rank=res['spearman_rho'].rank(method='first'))
          .nsmallest(4, 'spearman_rho'))['seg'].tolist()

df_seg = df.copy()
df_seg['seg'] = df_seg['Product_Name'].astype(str) + ' | ' + df_seg['Storage_Condition'].astype(str)
df_seg = df_seg[df_seg['seg'].isin(top4)]

g = sns.lmplot(
    data=df_seg.dropna(subset=['Shelf_Life_Days','Waste_Rate']),
    x='Shelf_Life_Days', y='Waste_Rate', col='seg', col_wrap=2,
    height=3.2, scatter_kws={'s':15, 'alpha':0.5}, order=2 # 2차 다항선으로 추세만 확인
)
for ax in g.axes.flat:
    ax.set_xscale('log'); ax.grid(alpha=0.3, which='both')
g.set_titles('{col_name}'); g.set_axis_labels('Shelf Life Days (log)', 'Waste Rate')
plt.tight_layout(); plt.show()

# 로그 기준 균등 간격 구간 생성 (보기용)
q = np.quantile(np.log1p(df1['Shelf_Life_Days']), [0, .25, .5, .75, 1])
bins = np.exp(q) - 1
labels = [f'{int(a)}–{int(b)}d' for a,b in zip(bins[:-1], bins[1:])]

d2 = df1.copy()
d2['SL_bin'] = pd.cut(d2['Shelf_Life_Days'], bins=bins, labels=labels, include_lowest=True)

plt.figure(figsize=(8,5))
sns.boxplot(data=d2, x='SL_bin', y='Waste_Rate', showfliers=False, color='cornflowerblue')
sns.pointplot(data=d2, x='SL_bin', y='Waste_Rate', estimator=np.mean, errorbar=None,
              color='red', markers='D', linestyles='--')
plt.title('유통기한 구간별 폐기율 분포', fontsize=12)
plt.xlabel('Shelf Life 구간'); plt.ylabel('Waste Rate')
plt.grid(axis='y', alpha=0.3); plt.tight_layout(); plt.show()

import statsmodels.formula.api as smf
# 선형
m1 = smf.ols('Waste_Rate ~ Shelf_Life_Days', data=df).fit()
print(m1.summary())

# 비선형(스플라인 3개 절점 예)
from patsy import dmatrix
import statsmodels.api as sm
X = dmatrix("bs(Shelf_Life_Days, df=5, degree=3, include_intercept=False)", df, return_type='dataframe')
m2 = sm.OLS(df['Waste_Rate'], sm.add_constant(X)).fit()
print(m2.summary())

# 2차 다항식(곡선) 회귀선
sns.lmplot(data=df, x="Shelf_Life_Days", y="Waste_Rate", order=2, ci=95, height=6, aspect=1.2)

plt.title("Shelf Life Days vs Waste Rate (Polynomial Fit)", fontsize=14)
plt.xlabel("Shelf Life Days")
plt.ylabel("Waste Rate")
plt.show()

"""---
머신러닝을 위한 전처리_스케일링   
선형회귀를 위해 스케일링 실시   
   
1. 문자열 데이터 변환
- 스케일러는 대개 수치형 데이터만 처리 가능, 문자열 데이터는 수치형으로 전환해주어야 함.
- 문자열을 숫자로 변환, 순서형일 경우 Label Encoding, 범주 구분용일 경우 One-Hot encoding
"""

df.describe()

df.columns

df.select_dtypes(include=['object', 'category']).columns

df_encoded = pd.get_dummies(df, columns=['Location', 'Product_Name', 'Brand', 'Storage_Condition',
       'Customer_Location', 'Sales_Channel'], drop_first=True)

df = pd.get_dummies(df, columns=['Location', 'Product_Name', 'Brand', 'Storage_Condition',
       'Customer_Location', 'Sales_Channel'], drop_first=True)

# 날짜를 수치형으로
df["Date"] = df["Date"].astype("int64") // 10**9   # 초 단위
df["Production_Date"] = df["Production_Date"].astype("int64") // 10**9
df["Expiration_Date"] = df["Expiration_Date"].astype("int64") // 10**9

print(df.dtypes)

df = df.astype({col: 'int' for col in df.select_dtypes(include='bool').columns})

is_numeric = np.issubdtype(df.dtypes.values[0], np.number)  # 첫 칼럼 확인
all_numeric = np.all([np.issubdtype(dtype, np.number) for dtype in df.dtypes])
print("모든 칼럼이 수치형인가?", all_numeric)

df.columns

from sklearn.model_selection import train_test_split

X = df.drop([ 'Product_ID', 'Waste_Rate','Expected_Waste_Qty', 'Quantity_in_Stock', 'Quantity_Sold', 'Total_Value',
             'Shelf_Life_Days', 'Remaining_Shelf_Life', 'Avg_Daily_Sales', 'Sales_Period_Days',
             'Date', 'Minimum_Stock_Threshold', 'Reorder_Quantity', 'Total_Land_Area', 'Approx_Total_Revenue',
             'Price_per_Unit_Sold'], axis=1)  # 독립변수
y = df["Waste_Rate"]               # 종속변수

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X.columns

from sklearn.model_selection import cross_validate
from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(n_estimators=100, random_state=42)

scores = cross_validate(rf, X_train, y_train, cv=5, return_train_score=True)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))

from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(
    n_estimators=500,
    max_depth=10,           # 깊이 제한
    min_samples_split=10,   # 노드 분할 최소 샘플 수
    min_samples_leaf=5,     # 리프 최소 샘플 수
    max_features="sqrt",    # 각 split에서 사용할 변수 개수
    random_state=42,
    n_jobs=-1
)
rf.fit(X_train, y_train)
print("Train R²:", rf.score(X_train, y_train))
print("Test  R²:", rf.score(X_test, y_test))

"""데이터 특성 자체

폐기율은 단순히 "유통기한"이나 "생산량" 같은 기본 변수만으로 설명되지 않음.

판매량, 날씨, 지역 이벤트, 유통망 지연 등 외부 요인이 큼 → 설명변수 부족.

데이터 분포

대부분 낮은 폐기율에 몰려 있고, 특정 구간만 극단적으로 높음 → 모델이 평균치만 따라가 버림.

결과적으로 RMSE는 낮아도 R²는 낮게 나올 수 있음.

데이터 누수 방지 때문에 X가 제한적

미래값(실제 판매량, 폐기량)을 못 쓰고, 생산 시점 변수만 쓰면 정보량이 제한됨.

현실적으로 완벽한 예측보다는 “대략적인 위험 구간” 분류가 더 현실적일 때가 많음.

변수 중요도 확인 후 불필요한 변수 제거   
importances = pd.Series(rf.feature_importances_, index=X_train.columns)
print(importances.sort_values(ascending=False).head(20))
"""

importances = pd.Series(rf.feature_importances_, index=X_train.columns)
print(importances.sort_values(ascending=False).head(20))

!pip install xgboost lightgbm catboost

# =========================
# 0) 공통 준비
# =========================
import numpy as np
import pandas as pd

from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.model_selection import KFold, cross_validate
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# (옵션) bool -> int 변환
for col in X_train.select_dtypes(include='bool').columns:
    X_train[col] = X_train[col].astype(int)
    X_test[col]  = X_test[col].astype(int)

cv = KFold(n_splits=5, shuffle=True, random_state=42)
RANDOM_STATE = 42

# =========================
# 1) 모델 정의
# =========================
models = {}

# 1) RandomForest
from sklearn.ensemble import RandomForestRegressor
models["RandomForest"] = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("model", RandomForestRegressor(
        n_estimators=500,
        max_depth=12,
        min_samples_split=10,
        min_samples_leaf=4,
        max_features="sqrt",
        n_jobs=-1,
        random_state=RANDOM_STATE
    ))
])

# 2) XGBoost (설치되어 있으면 추가)
try:
    from xgboost import XGBRegressor
    models["XGBoost"] = Pipeline([
        ("imputer", SimpleImputer(strategy="median")),
        ("model", XGBRegressor(
            n_estimators=1000,
            learning_rate=0.05,
            max_depth=6,
            subsample=0.8,
            colsample_bytree=0.8,
            reg_lambda=1.0,
            random_state=RANDOM_STATE,
            n_jobs=-1,
            tree_method="hist"
        ))
    ])
except Exception as e:
    print(f"[정보] XGBoost 건너뜀: {e}")

# 3) LightGBM (설치되어 있으면 추가)
try:
    from lightgbm import LGBMRegressor, early_stopping, log_evaluation
    models["LightGBM"] = Pipeline([
        ("imputer", SimpleImputer(strategy="median")),
        ("model", LGBMRegressor(
            n_estimators=3000,
            learning_rate=0.03,
            max_depth=-1,
            num_leaves=31,
            subsample=0.8,
            colsample_bytree=0.8,
            reg_lambda=1.0,
            random_state=RANDOM_STATE,
            n_jobs=-1,
            verbosity=-1  # <- fit(verbose=...) 대신 파라미터로 조정
        ))
    ])
except Exception as e:
    print(f"[정보] LightGBM 건너뜀: {e}")

# 4) CatBoost (설치되어 있으면 추가)
try:
    from catboost import CatBoostRegressor
    models["CatBoost"] = Pipeline([
        ("imputer", SimpleImputer(strategy="median")),
        ("model", CatBoostRegressor(
            iterations=3000,
            learning_rate=0.03,
            depth=8,
            l2_leaf_reg=3.0,
            random_state=RANDOM_STATE,
            loss_function="RMSE",
            verbose=False  # fit에서 verbose 안 씁니다(조용히)
        ))
    ])
except Exception as e:
    print(f"[정보] CatBoost 건너뜀: {e}")

# =========================
# 2) 평가 함수 (CV + Test + 중요도)
# =========================
def evaluate_model(name, pipe, X_train, y_train, X_test, y_test):
    print(f"\n===== {name} =====")

    # 2-1) KFold 교차검증 (R2/RMSE/MAE)
    scores = cross_validate(
        pipe, X_train, y_train, cv=cv, n_jobs=-1,
        scoring=["r2", "neg_mean_squared_error", "neg_mean_absolute_error"],
        return_train_score=False
    )
    r2_mean, r2_std = scores["test_r2"].mean(), scores["test_r2"].std()
    rmse_arr = np.sqrt(-scores["test_neg_mean_squared_error"])
    mae_arr  = -scores["test_neg_mean_absolute_error"]
    print(f"[CV]   R²   : {r2_mean:.4f} ± {r2_std:.4f}")
    print(f"[CV]   RMSE : {rmse_arr.mean():.4f} (± {rmse_arr.std():.4f})")
    print(f"[CV]   MAE  : {mae_arr.mean():.4f} (± {mae_arr.std():.4f})")

    # 2-2) 홀드아웃 테스트 (부스팅류는 early stopping 적용)
    pipe_fit = pipe
    last_step = pipe.steps[-1][1]
    model_type = str(type(last_step)).lower()

    if any(kw in model_type for kw in ["xgbregressor", "lgbmregressor", "catboostregressor"]):
        # 파이프라인 분리: imputer로 변환 후 모델 단독 학습
        imputer = pipe.named_steps["imputer"]
        Xtr = imputer.fit_transform(X_train)
        Xte = imputer.transform(X_test)

        model = last_step
        # XGBoost
        if "xgbregressor" in model_type:
            model.set_params(early_stopping_rounds=100)
            model.fit(Xtr, y_train, eval_set=[(Xte, y_test)], verbose=False)
        # LightGBM (callbacks 사용, verbose 인자 사용 안 함)
        elif "lgbmregressor" in model_type:
            model.set_params(early_stopping_rounds=100)
            model.fit(
                Xtr, y_train,
                eval_set=[(Xte, y_test)],
                callbacks=[
                    early_stopping(stopping_rounds=100),
                    log_evaluation(period=0)  # 로그 억제
                ]
            )
        # CatBoost
        elif "catboostregressor" in model_type:
            model.set_params(early_stopping_rounds=100)
            model.fit(Xtr, y_train, eval_set=(Xte, y_test), verbose=False)

        pred = model.predict(Xte)
        fitted = (imputer, model)
    else:
        # RandomForest 등
        pipe_fit.fit(X_train, y_train)
        pred = pipe_fit.predict(X_test)
        fitted = pipe_fit

    # 2-3) 테스트 성능
    mse  = mean_squared_error(y_test, pred)
    rmse = np.sqrt(mse)
    mae  = mean_absolute_error(y_test, pred)
    r2   = r2_score(y_test, pred)
    print(f"[TEST] R²   : {r2:.4f}")
    print(f"[TEST] RMSE : {rmse:.4f}")
    print(f"[TEST] MAE  : {mae:.4f}")

    # 2-4) 피처 중요도 (가능할 때만)
    try:
        feature_names = X_train.columns
        if isinstance(fitted, tuple):  # (imputer, model)
            _, model = fitted
            if hasattr(model, "feature_importances_"):
                importances = pd.Series(model.feature_importances_, index=feature_names)
            else:
                importances = None
        else:  # Pipeline
            model = fitted.named_steps["model"]
            importances = pd.Series(model.feature_importances_, index=feature_names) \
                          if hasattr(model, "feature_importances_") else None

        if importances is not None:
            top = importances.sort_values(ascending=False).head(15).round(4)
            print("\n[Top-15 Feature Importances]")
            print(top)
    except Exception as e:
        print(f"(중요도 출력 생략: {e})")

# =========================
# 3) 실행
# =========================
for name, pipe in models.items():
    evaluate_model(name, pipe, X_train, y_train, X_test, y_test)

"""최우수 모델: CatBoost

CV: R² 0.722 ± 0.013, RMSE 0.172, MAE 0.114   

Test: R² 0.769, RMSE 0.154, MAE 0.111   
→ 전반적으로 오차가 가장 작고 일반화 성능이 최고입니다.   
   
차선책: RandomForest   

CV: R² 0.705, RMSE 0.177, MAE 0.136   

Test: R² 0.724, RMSE 0.169, MAE 0.131   
→ 안정적이지만 CatBoost보다 오차가 더 큼.   

XGBoost는 RF와 비슷하거나 약간 열세, LightGBM은 현재 설정에선 열위입니다.   

---

##1) 일반화 성능 (CV ↔ Test)

CatBoost: CV와 Test가 근접(0.722 → 0.769)하고 Test가 더 좋음    
→ 현재 분할에서 과소적합 없이 일반화 양호.   
   
RandomForest/XGBoost: CV와 Test가 대체로 일관적 → 안정적.   
   
LightGBM: CV 0.575 → Test 0.631로 갭이 크고 전반적으로 낮음.   
   
원인 가능성: (a) 파라미터가 데이터 특성에 비적합, (b) 변수가 적고 상호작용을 덜 잡음, (c) fit 시 경고(특징명 불일치)로 인한 미묘한 영향.   

##2) 오차 규모   
CatBoost Test RMSE ≈ 0.154, MAE ≈ 0.111   
→ RandomForest 대비 RMSE ~9% 감소, MAE ~15% 감소 수준(대략).   
→ 실무 관점에서 의미 있는 개선.

## 3) 피쳐 중요도 공통점

세 모델 공통 상위: Approx_Total_Revenue > Total_Value > Shelf_Life   

→ 목표변수(Waste_Rate)에 매출/가치 관련 지표가 강하게 연결되어 있음을 시사.   

→ 단, 이것이 데이터 누수(leakage) 는 아닌지 반드시 점검 필요:
   
Waste_Rate가 폐기량/생산(or 재고) 방식이라면 매출/가치 변수와 직접산식 중복은 아닐 수 있으나,
   
매출이 판매량·재고회전·수요 강도의 강력한 proxy라면, Waste_Rate 예측에 매우 유리하게 작용(과도한 상관)할 수 있습니다.   
→ 권장: 매출·가치 계열 변수를 제거/축소한 소거 실험(ablations) 으로 실제 기여도/누수 가능성 확인.

참고: LightGBM 중요도 값이 12822, 10359처럼 정규화되지 않은 카운트형(분할기여 count) 으로 보이는 건 정상입니다. 모델마다 척도가 달라 절대값 비교가 아닌 순위만 보세요.
"""

df.columns

import numpy as np, pandas as pd
from sklearn.model_selection import TimeSeriesSplit, cross_validate
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, precision_recall_curve, classification_report, confusion_matrix


# 0) 라벨링 (예: 15% 이상 고위험)
df2 = df.sort_values("Production_Date")  # 실제 생산일 컬럼명으로 정렬
y_cls = (df["Waste_Rate"] >= 0.20).astype(int)

features = ['Total_Land_Area', 'Number_of_Cows', 'Date', 'Production_Date', 'Brand', 'Quantity', 'Price_per_Unit',
            'Product_Name', 'Shelf_Life', 'Storage_Condition', 'Location']  # 필요에 따라 조정
X = df[features].copy()

num_cols = X.select_dtypes(include=[np.number]).columns.tolist()
cat_cols = [c for c in X.columns if c not in num_cols]

pre = ColumnTransformer(
    transformers=[
        ("num", SimpleImputer(strategy="median"), num_cols),
        ("cat", Pipeline([
            ("imp", SimpleImputer(strategy="most_frequent")),
            ("ohe", OneHotEncoder(handle_unknown="ignore"))
        ]), cat_cols),
    ],
    remainder="drop"
)

# 모델들
from sklearn.ensemble import RandomForestClassifier
try:
    from catboost import CatBoostClassifier
    use_cat = True
except:
    use_cat = False

try:
    from xgboost import XGBClassifier
    use_xgb = True
except:
    use_xgb = False

models = {
    "RF": RandomForestClassifier(
        n_estimators=500, max_depth=None, min_samples_split=5, min_samples_leaf=2,
        class_weight="balanced", n_jobs=-1, random_state=42
    )
}
if use_cat:
    models["CatBoost"] = CatBoostClassifier(
        iterations=2000, learning_rate=0.05, depth=8, l2_leaf_reg=3,
        loss_function="Logloss", eval_metric="AUC", random_seed=42,
        verbose=False, auto_class_weights="Balanced"
    )
if use_xgb:
    models["XGB"] = XGBClassifier(
        n_estimators=1500, learning_rate=0.05, max_depth=6,
        subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,
        objective="binary:logistic", eval_metric="auc", n_jobs=-1, random_state=42
        # 불균형일 때: scale_pos_weight = (neg/pos)
    )

def evaluate_temporal(model_name, model, X, y):
    print(f"\n===== {model_name} =====")
    pipe = Pipeline([("pre", pre), ("clf", model)])
    # 시간 분할(예: 5등분)
    tscv = TimeSeriesSplit(n_splits=5)
    aucs, pr_aucs = [], []
    for fold, (tr, te) in enumerate(tscv.split(X), 1):
        pipe.fit(X.iloc[tr], y.iloc[tr])
        proba = pipe.predict_proba(X.iloc[te])[:,1]
        aucs.append(roc_auc_score(y.iloc[te], proba))
        pr_aucs.append(average_precision_score(y.iloc[te], proba))
    print(f"[CV] ROC-AUC: {np.mean(aucs):.3f} ± {np.std(aucs):.3f}")
    print(f"[CV] PR-AUC : {np.mean(pr_aucs):.3f} ± {np.std(pr_aucs):.3f}")

    # 최종 홀드아웃(가장 최신 20%를 테스트로)
    n = len(X)
    split = int(n*0.8)
    pipe.fit(X.iloc[:split], y.iloc[:split])
    y_proba = pipe.predict_proba(X.iloc[split:])[:,1]
    y_true  = y.iloc[split:]

    # 비용가중 임계값(예: FN 비용=5, FP 비용=1)
    fn_cost, fp_cost = 5.0, 1.0
    precisions, recalls, thresholds = precision_recall_curve(y_true, y_proba)
    costs = []
    for th in np.linspace(0.05, 0.95, 19):
        y_hat = (y_proba >= th).astype(int)
        tn, fp, fn, tp = confusion_matrix(y_true, y_hat).ravel()
        cost = fn_cost*fn + fp_cost*fp
        costs.append((th, cost))
    best_th, _ = min(costs, key=lambda x: x[1])

    y_pred = (y_proba >= best_th).astype(int)
    print(f"[TEST] ROC-AUC: {roc_auc_score(y_true, y_proba):.3f} | PR-AUC: {average_precision_score(y_true, y_proba):.3f}")
    print(f"[TEST] Best threshold by cost: {best_th:.2f}")
    print(classification_report(y_true, y_pred, digits=3))
    print("Confusion matrix (TN FP; FN TP):")
    print(confusion_matrix(y_true, y_pred))

for name, model in models.items():
    evaluate_temporal(name, model, X, y_cls)

X.select_dtypes(include=['object']).columns

bool_cols = X.select_dtypes(include='bool').columns
X[bool_cols] = X[bool_cols].astype(np.int8)   # 또는 'int64'

df_encoded = pd.get_dummies(df, columns=['Location', 'Product_Name', 'Brand', 'Storage_Condition',
       'Customer_Location', 'Sales_Channel'], drop_first=True)

df_encoded

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df['Storage_Condition_encoded'] = le.fit_transform(df['Storage_Condition'])

df_X = df[['Quantity', 'Price_per_Unit', 'Shelf_Life', 'Storage_Condition_encoded']]

import pandas as pd
import numpy as np

# 구간 정의 (0~40%까지 4등급, 나머지는 -1로 표시)
bins = [0, 0.10, 0.20, 0.30, 0.40]
labels = [0, 1, 2, 3]

df['Waste_Class'] = pd.cut(df['Waste_Rate'], bins=bins, labels=labels, include_lowest=True).astype(float)

bins   = [0, 0.10, 0.20, 0.30, 0.40, 1.01]  # 1.0까지 커버(+ε)
labels = [0, 1, 2, 3, 4]                    # 5등급
y = pd.cut(df['Waste_Rate'], bins=bins, labels=labels,
           include_lowest=True, right=False).astype(int)

feature_cols = ['Quantity','Price_per_Unit','Shelf_Life'] + \
               [c for c in df_encoded.columns if c.startswith('Storage_Condition_')]

X = df_encoded[feature_cols]
y = df['Waste_Class']   # 다중 분류 타깃

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import accuracy_score, roc_auc_score

# 0) 원본 df와 인코딩된 df 정렬 일치
df = df.sort_index()
df_encoded = df_encoded.sort_index()

# 1) y 생성 전, Waste_Rate 결측 행 제거
df_clean = df[~df['Waste_Rate'].isna()].copy()

# 2) y 이진 라벨 (nullable Int64 대신 꼭 int로)
y = (df_clean['Waste_Rate'].astype(float) >= 0.20).astype(np.int64)

# 3) X 구성: df_encoded도 df_clean의 행만 유지(인덱스 정렬 중요)
df_enc_aligned = df_encoded.loc[df_clean.index].copy()

# 필요 컬럼만(존재하는 것만 안전 선택)
base_cols = [c for c in ['Quantity','Price_per_Unit','Shelf_Life'] if c in df_enc_aligned.columns]
sc_cols = [c for c in df_enc_aligned.columns if c.startswith('Storage_Condition_')]
X = df_enc_aligned[base_cols + sc_cols].copy()

# 4) X의 숫자형만 남기고, 남은 NaN은 임시 대체(스케일러/SGD가 NaN을 못받음)
num_cols = X.select_dtypes(include=[np.number]).columns.tolist()
X = X[num_cols].fillna(0.0)  # 간단 대체. 더 나은 방법: SimpleImputer(strategy='median')

# 5) y에 NaN 없는지 최종 검증 (예외 방지)
assert y.isna().sum() == 0, "y에 NaN이 남아있습니다."

# 6) 분할/스케일/학습
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y  # 클래스 불균형 시 stratify 권장
)

ss = StandardScaler()
X_train_s = ss.fit_transform(X_train)
X_test_s  = ss.transform(X_test)

clf = SGDClassifier(loss='log_loss', max_iter=1000, random_state=42)
clf.fit(X_train_s, y_train)

print("Train acc:", clf.score(X_train_s, y_train))
print("Test  acc:", clf.score(X_test_s, y_test))
print("Test AUC:", roc_auc_score(y_test, clf.predict_proba(X_test_s)[:,1]))

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

ss = StandardScaler()
X_train_s = ss.fit_transform(X_train)
X_test_s  = ss.transform(X_test)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

clf = LogisticRegression(multi_class='multinomial', max_iter=1000, random_state=42)
clf.fit(X_train_s, y_train)

y_pred = clf.predict(X_test_s)

print("Accuracy:", clf.score(X_test_s, y_test))
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

"""실제 클래스 0 → 105개만 맞춤, 225개는 잘못 예측해서 클래스 1로 분류

실제 클래스 1 → 493개 맞춤, 42개만 잘못 예측해서 클래스 0으로 분류

→ 모델이 클래스 1 쪽으로 강하게 편향되어 있음. (Recall 0.92 vs Recall 0.32)

전반 정확도(69%)는 그럭저럭 보이지만, 클래스 불균형이 심각함.

모델이 "클래스 1(10~20%)"을 주로 예측하면서 Recall은 높고, 클래스 0은 거의 잡지 못함.

즉, 특정 구간 예측(0~10%)에는 실패하고, 10~20% 구간 중심으로 예측하는 모델이 됨.

"""

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=200, random_state=42)
rf.fit(X_train, y_train)
print("RF Accuracy:", rf.score(X_test, y_test))

df.columns

# 생산 시점에 알 수 있는 변수만 선택
use_cols = ['Date','Product_Name', 'Brand', 'Quantity',
             'Price_per_Unit', 'Shelf_Life', 'Storage_Condition', 'Production_Date','Expiration_Date']

X_raw = df[use_cols]
y = df["Waste_Rate"]   # 연속값 (회귀) or 구간화 (분류)

X = pd.get_dummies(X_raw, drop_first=True)   # 원-핫 인코딩

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

rf = RandomForestRegressor(
    n_estimators=300,
    max_depth=None,
    random_state=42,
    n_jobs=-1
)
rf.fit(X_train, y_train)

pred = rf.predict(X_test)

from sklearn.metrics import mean_absolute_error, r2_score

print("R²:", r2_score(y_test, pred))
print("MAE:", mean_absolute_error(y_test, pred))

# 구간화 (예: 0~10%, 10~20%, 20~30%, 30~40%)
bins = [0, 0.10, 0.20, 0.30, 0.40, 1.01]
labels = [0,1,2,3,4]   # 5등급
y_class = pd.cut(y, bins=bins, labels=labels, include_lowest=True).astype(int)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

rf_cls = RandomForestClassifier(
    n_estimators=300,
    class_weight="balanced",
    random_state=42,
    n_jobs=-1
)
rf_cls.fit(X_train, y_class.loc[y_class.index.isin(X_train.index)])

y_pred = rf_cls.predict(X_test)
print(classification_report(y_class.loc[X_test.index], y_pred))

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_class.loc[y_class.index.isin(X_train.index)])
print(dt.score(X_train, y_class))
print(dt.score(X_test, y_class))

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

# ----------------------------------
# 1) 데이터 준비
# ----------------------------------
# (예시) 생산 시점에 알 수 있는 변수만 사용
use_cols = [
    "Brand", "Product_Name", "Storage_Condition",
    "Shelf_Life", "Production_Date", "Location",
    "Quantity", "Price_per_Unit"
]
X_raw = df[use_cols].copy()
y_raw = df["Waste_Rate"].copy()

# ----------------------------------
# 2) 파생변수 추가 (Feature Engineering)
# ----------------------------------
# 날짜 기반 변수
X_raw["Production_Date"] = pd.to_datetime(X_raw["Production_Date"], errors="coerce")
X_raw["prod_month"] = X_raw["Production_Date"].dt.month
X_raw["prod_dayofweek"] = X_raw["Production_Date"].dt.dayofweek
X_raw.drop(columns=["Production_Date"], inplace=True)

# 상호작용 변수
X_raw["qty_per_shelf"] = X_raw["Quantity"] / (X_raw["Shelf_Life"] + 1)

# ----------------------------------
# 3) 범주형 인코딩
# ----------------------------------
X = pd.get_dummies(X_raw, drop_first=True)

# ----------------------------------
# 4) train/test 분리
# ----------------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y_raw, test_size=0.2, random_state=42
)

# 로그 변환 버전 (극단치 완화)
y_train_log = np.log1p(y_train)
y_test_log  = np.log1p(y_test)

# ----------------------------------
# 5) 모델 정의
# ----------------------------------
models = {
    "RandomForest": RandomForestRegressor(
        n_estimators=300, random_state=42, n_jobs=-1
    ),
    "XGBoost": XGBRegressor(
        n_estimators=300, learning_rate=0.1, max_depth=6,
        subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1
    ),
    "LightGBM": LGBMRegressor(
        n_estimators=300, learning_rate=0.1, max_depth=-1,
        subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1
    )
}

# ----------------------------------
# 6) 평가 함수
# ----------------------------------
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# 버전 안전한 RMSE 계산기
def RMSE(y_true, y_pred):
    try:
        # 신버전이면 이 경로(있다면)
        return mean_squared_error(y_true, y_pred, squared=False)
    except TypeError:
        # 구버전 호환: sqrt(MSE)
        return np.sqrt(mean_squared_error(y_true, y_pred))

def evaluate_model(name, model, X_train, X_test, y_train, y_test, log=False):
    model.fit(X_train, y_train)
    pred = model.predict(X_test)

    # 로그 타깃 사용 시 역변환
    if log:
        pred = np.expm1(pred)
        y_test = np.expm1(y_test)

    r2   = r2_score(y_test, pred)
    rmse = RMSE(y_test, pred)          # ← 여기만 변경
    mae  = mean_absolute_error(y_test, pred)

    print(f"=== {name} (log={log}) ===")
    print(f"R²   : {r2:.4f}")
    print(f"RMSE : {rmse:.4f}")
    print(f"MAE  : {mae:.4f}")
    print("-"*40)

# ----------------------------------
# 7) 실행
# ----------------------------------
for name, model in models.items():
    # 원본 y
    evaluate_model(name, model, X_train, X_test, y_train, y_test, log=False)
    # 로그 변환 y
    evaluate_model(name, model, X_train, X_test, y_train_log, y_test_log, log=True)

# ----------------------------------
# 8) 중요 피처 확인 (예: 랜덤포레스트)
# ----------------------------------
rf = models["RandomForest"].fit(X_train, y_train)
feat_imp = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)
print("\n[상위 10개 중요 변수]")
print(feat_imp.head(10))

import pandas as pd
import numpy as np

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score

# 1) 사용할 칼럼(생산 시점에 확정되는 것만)
use_cols = [
    "Brand","Product_Name","Storage_Condition",
    "Shelf_Life","Production_Date","Location",
    "Quantity","Price_per_Unit"
]
X_raw = df[use_cols].copy()

# 2) 파생변수 (미래 정보 사용 금지)
X_raw["Production_Date"] = pd.to_datetime(X_raw["Production_Date"], errors="coerce")
X_raw["month"] = X_raw["Production_Date"].dt.month
X_raw["dow"] = X_raw["Production_Date"].dt.dayofweek
X_raw["is_weekend"] = (X_raw["dow"] >= 5).astype(int)
X_raw.drop(columns=["Production_Date"], inplace=True)

X_raw["qty_per_shelf"] = X_raw["Quantity"] / (X_raw["Shelf_Life"] + 1)

# 3) 인코딩(원-핫) + 스케일링
X_numcat = pd.get_dummies(X_raw, drop_first=True)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_numcat)

# 4) (선택) PCA로 노이즈/차원 축소
pca = PCA(n_components=0.9, svd_solver="full")  # 누적분산 90% 유지
X_pca = pca.fit_transform(X_scaled)

# 5) k 탐색 (엘보우+실루엣)
results = []
for k in range(2, 11):
    km = KMeans(n_clusters=k, n_init=20, random_state=42)
    labels = km.fit_predict(X_pca)
    sil = silhouette_score(X_pca, labels)
    db  = davies_bouldin_score(X_pca, labels)
    results.append((k, sil, db))
best_k = max(results, key=lambda t: t[1])[0]
print("k 탐색(실루엣 최대):", results, "=> best_k:", best_k)

# 6) 최종 모델 적합
kmeans = KMeans(n_clusters=best_k, n_init=50, random_state=42)
clusters = kmeans.fit_predict(X_pca)

# 7) 해석용 결합(타깃은 학습에 미사용) → 사후 프로파일링
out = df.copy()
out["cluster"] = clusters

# 군집별 프로파일(입력 특성)
profile_inputs = (out
  .groupby("cluster")
  .agg({
      "Shelf_Life": ["mean","median"],
      "Quantity": ["mean","median"],
      "Price_per_Unit": ["mean","median"],
      "Brand": lambda s: s.value_counts().index[0],           # 최빈
      "Product_Name": lambda s: s.value_counts().index[0],
      "Storage_Condition": lambda s: s.value_counts().index[0],
      "Location": lambda s: s.value_counts().index[0]
  })
)
print(profile_inputs)

# (선택) 군집별 폐기율(타깃) 분포 비교: 학습엔 미사용, 해석만
if "Waste_Rate" in out.columns:
    wr_summary = out.groupby("cluster")["Waste_Rate"].describe().round(3)
    print("\n[군집별 Waste_Rate 요약]\n", wr_summary)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, silhouette_samples

# -------------------------------
# 0) 설정
# -------------------------------
USE_COLS = [
    "Brand","Product_Name","Storage_Condition",
    "Shelf_Life","Production_Date","Location",
    "Quantity","Price_per_Unit"
]
TOP_K_CAT = 5               # 범주형 Top-k 막대그래프
K_RANGE   = range(2, 11)    # k 탐색 범위

# -------------------------------
# 1) 전처리 (누수 방지)
# -------------------------------
X_raw = df[USE_COLS].copy()

# 날짜 파생 (생산 시점 정보만)
X_raw["Production_Date"] = pd.to_datetime(X_raw["Production_Date"], errors="coerce")
X_raw["month"] = X_raw["Production_Date"].dt.month
X_raw["dow"] = X_raw["Production_Date"].dt.dayofweek
X_raw["is_weekend"] = (X_raw["dow"] >= 5).astype(int)
X_raw.drop(columns=["Production_Date"], inplace=True)

# 상호작용/밀도 지표
X_raw["qty_per_shelf"] = X_raw["Quantity"] / (X_raw["Shelf_Life"] + 1)

# 원-핫 인코딩
X = pd.get_dummies(X_raw, drop_first=True)

# 스케일링
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# -------------------------------
# 2) PCA 변환 (시각화/노이즈 완화)
# -------------------------------
pca = PCA(n_components=0.9, svd_solver="full")  # 누적분산 90%
X_pca = pca.fit_transform(X_scaled)

# -------------------------------
# 3) k 선택: 실루엣 최고값
# -------------------------------
sil_results = []
for k in K_RANGE:
    km = KMeans(n_clusters=k, n_init=20, random_state=42)
    labels = km.fit_predict(X_pca)
    sil = silhouette_score(X_pca, labels)
    sil_results.append((k, sil))
best_k, best_sil = max(sil_results, key=lambda t: t[1])
print("[k 탐색 결과] (k, silhouette):", sil_results)
print(f"[선택된 k] {best_k} (silhouette={best_sil:.3f})")

# 최종 적합
kmeans = KMeans(n_clusters=best_k, n_init=50, random_state=42)
labels = kmeans.fit_predict(X_pca)

# 원본 DF에 군집 부착 (사후 해석용; 학습에는 미사용)
out = df.copy()
out["cluster"] = labels

# -------------------------------
# 4) 시각화
# -------------------------------

# (A) PCA 2D 산점도 (클러스터)
plt.figure(figsize=(7,6))
for c in np.unique(labels):
    mask = labels == c
    plt.scatter(X_pca[mask, 0], X_pca[mask, 1], s=12, alpha=0.7, label=f"Cluster {c}")
plt.title("PCA 2D Scatter by Cluster (PCA 2차원 산점도)")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.legend()
plt.tight_layout()
plt.show()

# (B) 실루엣 다이어그램
sil_vals = silhouette_samples(X_pca, labels)
y_lower = 10
plt.figure(figsize=(7,6))
for c in range(best_k):
    c_sil = sil_vals[labels == c]
    c_sil.sort()
    y_upper = y_lower + len(c_sil)
    plt.fill_betweenx(np.arange(y_lower, y_upper), 0, c_sil, alpha=0.7)
    plt.text(-0.05, y_lower + 0.5 * len(c_sil), str(c))
    y_lower = y_upper + 10
plt.axvline(x=best_sil, linestyle="--")
plt.title(f"Silhouette Plot (k={best_k}, mean={best_sil:.3f})")
plt.xlabel("Silhouette coefficient")
plt.ylabel("Cluster")
plt.tight_layout()
plt.show()

# (C) 군집 × 수치 특성 평균(표준화) 히트맵
# 수치열 선택 (파생 포함)
num_cols = ["Shelf_Life","Quantity","Price_per_Unit","month","dow","is_weekend","qty_per_shelf"]
num_cols = [c for c in num_cols if c in out.columns]  # 존재 확인

prof_num = out.groupby("cluster")[num_cols].mean()
# 특성별 표준화(행간 비교를 위해 열 z-score)
prof_num_std = (prof_num - prof_num.mean(axis=0)) / (prof_num.std(axis=0) + 1e-9)

plt.figure(figsize=(1.6*len(num_cols)+3, 0.5*best_k+3))
im = plt.imshow(prof_num_std.values, aspect="auto")
plt.colorbar(im, fraction=0.025, pad=0.04)
plt.yticks(ticks=np.arange(prof_num_std.shape[0]), labels=[f"C{c}" for c in prof_num_std.index])
plt.xticks(ticks=np.arange(len(num_cols)), labels=num_cols, rotation=45, ha="right")
plt.title("Cluster × Numeric Feature (Z-scored Means)\n(군집×수치 특성 평균: 표준화)")
plt.tight_layout()
plt.show()

# (D) 군집별 범주형 Top-k 빈도 막대그래프
def plot_topk_bars(df_, cluster_col, cat_col, topk=TOP_K_CAT):
    clusters = sorted(df_[cluster_col].unique())
    for c in clusters:
        s = (df_.loc[df_[cluster_col]==c, cat_col]
             .value_counts()
             .head(topk))
        plt.figure(figsize=(6,3.5))
        plt.bar(s.index.astype(str), s.values)
        plt.title(f"Cluster {c} - Top {topk} {cat_col} (최다 빈도)")
        plt.ylabel("Count")
        plt.xticks(rotation=30, ha="right")
        plt.tight_layout()
        plt.show()

for cat_col in ["Brand","Storage_Condition"]:
    if cat_col in out.columns:
        plot_topk_bars(out, "cluster", cat_col, topk=TOP_K_CAT)

# (E) (선택) 군집별 폐기율 분포 박스플롯 (학습엔 미사용, 사후 해석용)
if "Waste_Rate" in out.columns:
    plt.figure(figsize=(7,5))
    data = [out.loc[out["cluster"]==c, "Waste_Rate"].dropna().values for c in sorted(out["cluster"].unique())]
    plt.boxplot(data, labels=[f"C{c}" for c in sorted(out["cluster"].unique())], showfliers=False)
    plt.title("Waste_Rate by Cluster (사후 해석용)")
    plt.ylabel("Waste_Rate")
    plt.tight_layout()
    plt.show()

# -------------------------------
# 5) 요약 리포트(텍스트)
# -------------------------------
sizes = out["cluster"].value_counts().sort_index()
print("\n[군집 크기]"); print(sizes.to_string())

if "Waste_Rate" in out.columns:
    wr_desc = out.groupby("cluster")["Waste_Rate"].describe().round(3)
    print("\n[군집별 Waste_Rate 요약]"); print(wr_desc.to_string())

from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
sil = silhouette_score(X_pca, labels)
ch  = calinski_harabasz_score(X_pca, labels)
db  = davies_bouldin_score(X_pca, labels)
print(sil, ch, db)

from sklearn.cluster import KMeans
from sklearn.metrics import normalized_mutual_info_score as NMI
import numpy as np

def cluster_once(X, k, seed):
    return KMeans(n_clusters=k, n_init=20, random_state=seed).fit_predict(X)

base = cluster_once(X_pca, best_k, 42)
scores = []
for s in range(50):
    idx = np.random.choice(len(X_pca), len(X_pca), replace=True)
    lab = cluster_once(X_pca[idx], best_k, s)
    scores.append(NMI(base[idx], lab))
print("NMI 안정성 평균:", np.mean(scores))

